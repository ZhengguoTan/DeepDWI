import os

import numpy as np
import matplotlib.pyplot as plt

DIR = os.path.dirname(os.path.realpath(__file__))

train_loss = [
    1.011386 ,
    0.991767 ,
    0.984118 ,
    0.979690 ,
    0.976605 ,
    0.974293 ,
    0.972522 ,
    0.971151 ,
    0.970065 ,
    0.969218 ,
    0.968511 ,
    0.967941 ,
    0.967436 ,
    0.967014 ,
    0.966672 ,
    0.966360 ,
    0.966089 ,
    0.965854 ,
    0.965640 ,
    0.965458 ,
    0.965287 ,
    0.965128 ,
    0.964979 ,
    0.964844 ,
    0.964726 ,
    0.964588 ,
    0.964490 ,
    0.964371 ,
    0.964278 ,
    0.964182 ,
    0.964121 ,
    0.964102 ,
    0.964089 ,
    0.964079 ,
    0.964068 ,
    0.964058 ,
    0.964047 ,
    0.964037 ,
    0.964026 ,
    0.964016 ,
    0.964005 ,
    0.963995 ,
    0.963984 ,
    0.963974 ,
    0.963963 ,
    0.963952 ,
    0.963942 ,
    0.963931 ,
    0.963920 ,
    0.963909 ,
    0.963898 ,
    0.963886 ,
    0.963875 ,
    0.963864 ,
    0.963852 ,
    0.963841 ,
    0.963829 ,
    0.963818 ,
    0.963806 ,
    0.963795 ,
    0.963787 ,
    0.963785 ,
    0.963783 ,
    0.963782 ,
    0.963781 ,
    0.963780 ,
    0.963779 ,
    0.963777 ,
    0.963776 ,
    0.963775 ,
    0.963774 ,
    0.963772 ,
    0.963771 ,
    0.963770 ,
    0.963769 ,
    0.963767 ,
    0.963766 ,
    0.963765 ,
    0.963763 ,
    0.963762 ,
    0.963761 ,
    0.963759 ,
    0.963758 ,
    0.963757 ,
    0.963755 ,
    0.963754 ,
    0.963752 ,
    0.963751 ,
    0.963750 ,
    0.963748 ,
    0.963748 ,
    0.963747 ,
    0.963747 ,
    0.963747 ,
    0.963747 ,
    0.963747 ,
    0.963747 ,
    0.963746 ,
    0.963746 ,
    0.963746
]

valid_loss = [
    0.995069 ,
    0.982606 ,
    0.977813 ,
    0.974984 ,
    0.972953 ,
    0.971372 ,
    0.970130 ,
    0.969151 ,
    0.968360 ,
    0.967711 ,
    0.967186 ,
    0.966756 ,
    0.966394 ,
    0.966078 ,
    0.965821 ,
    0.965596 ,
    0.965399 ,
    0.965231 ,
    0.965078 ,
    0.964942 ,
    0.964825 ,
    0.964718 ,
    0.964626 ,
    0.964545 ,
    0.964465 ,
    0.964397 ,
    0.964330 ,
    0.964262 ,
    0.964203 ,
    0.964152 ,
    0.964129 ,
    0.964117 ,
    0.964113 ,
    0.964108 ,
    0.964102 ,
    0.964097 ,
    0.964092 ,
    0.964087 ,
    0.964082 ,
    0.964076 ,
    0.964071 ,
    0.964066 ,
    0.964061 ,
    0.964056 ,
    0.964050 ,
    0.964045 ,
    0.964040 ,
    0.964035 ,
    0.964030 ,
    0.964024 ,
    0.964019 ,
    0.964014 ,
    0.964009 ,
    0.964003 ,
    0.963998 ,
    0.963993 ,
    0.963988 ,
    0.963983 ,
    0.963977 ,
    0.963972 ,
    0.963972 ,
    0.963971 ,
    0.963970 ,
    0.963970 ,
    0.963969 ,
    0.963969 ,
    0.963968 ,
    0.963968 ,
    0.963967 ,
    0.963966 ,
    0.963966 ,
    0.963965 ,
    0.963965 ,
    0.963964 ,
    0.963964 ,
    0.963963 ,
    0.963962 ,
    0.963962 ,
    0.963961 ,
    0.963961 ,
    0.963960 ,
    0.963959 ,
    0.963959 ,
    0.963958 ,
    0.963957 ,
    0.963957 ,
    0.963956 ,
    0.963956 ,
    0.963955 ,
    0.963954 ,
    0.963954 ,
    0.963954 ,
    0.963954 ,
    0.963954 ,
    0.963954 ,
    0.963954 ,
    0.963954 ,
    0.963954 ,
    0.963953 ,
    0.963953
]

lamda = [
    0.044764,
    0.041894,
    0.039967,
    0.038494,
    0.037261,
    0.036190,
    0.035245,
    0.034407,
    0.033660,
    0.032990,
    0.032385,
    0.031839,
    0.031343,
    0.030889,
    0.030474,
    0.030092,
    0.029741,
    0.029417,
    0.029117,
    0.028839,
    0.028582,
    0.028342,
    0.028120,
    0.027913,
    0.027720,
    0.027540,
    0.027373,
    0.027219,
    0.027077,
    0.026945,
    0.026933,
    0.026922,
    0.026912,
    0.026901,
    0.026890,
    0.026879,
    0.026869,
    0.026858,
    0.026847,
    0.026836,
    0.026824,
    0.026813,
    0.026802,
    0.026791,
    0.026780,
    0.026769,
    0.026757,
    0.026746,
    0.026735,
    0.026724,
    0.026713,
    0.026701,
    0.026690,
    0.026679,
    0.026668,
    0.026657,
    0.026646,
    0.026635,
    0.026624,
    0.026614,
    0.026613,
    0.026612,
    0.026611,
    0.026609,
    0.026608,
    0.026607,
    0.026606,
    0.026605,
    0.026604,
    0.026603,
    0.026602,
    0.026601,
    0.026599,
    0.026598,
    0.026597,
    0.026596,
    0.026595,
    0.026593,
    0.026592,
    0.026591,
    0.026590,
    0.026588,
    0.026587,
    0.026586,
    0.026585,
    0.026583,
    0.026582,
    0.026581,
    0.026579,
    0.026578,
    0.026578,
    0.026578,
    0.026578,
    0.026578,
    0.026577,
    0.026577,
    0.026577,
    0.026577,
    0.026577,
    0.026577
]

fontsize = 14

epochs = range(1, len(train_loss)+1, 1)

fig, ax1 = plt.subplots(figsize=(6,3))

color = 'tab:red'
ax1.set_xlabel('epochs', fontsize=fontsize)
# ax1.set_ylabel('loss', color=color)
ln1 = ax1.plot(epochs, train_loss, '-', color=color, label='training')
ln2 = ax1.plot(epochs, valid_loss, '--', color=color, label='validation')
ax1.tick_params(axis='y', labelcolor=color)

ax2 = ax1.twinx()  # instantiate a second Axes that shares the same x-axis

color = 'tab:blue'
# ax2.set_ylabel('$\lambda$', color=color)  # we already handled the x-label with ax1
ln3 = ax2.plot(epochs, lamda, color=color, label='$\lambda$')
ax2.tick_params(axis='y', labelcolor=color)
ax2.set_ylim([0.01, 0.05])

lns = ln1 + ln2 + ln3
lbs = [l.get_label() for l in lns]
ax1.legend(lns, lbs, loc=1, fontsize=fontsize)

fig.tight_layout()  # otherwise the right y-label is slightly clipped

plt.xlim([1, 100])
plt.title('ADMM Unrolling Convergence')

plt.subplots_adjust(wspace=0.0, hspace=0.0)
plt.savefig(DIR + '/convergence.png',
            bbox_inches='tight', pad_inches=0, dpi=300)

plt.show()
